{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Princeton Web Census Interface\n",
    "## About\n",
    "This jupyter notebook provides a demo for interacting with data from the Princeton Web Census.\n",
    "\n",
    "The code relies on our utilities located in the censuslib/ directory, which you can use for your own experiments.\n",
    "\n",
    "Each 'cell' in this notebook represents a different capability of our census interface. Each can be executed separately from the others, but first you must start with the following instructions...\n",
    "\n",
    "For questions or comments, feel free to email dill.reisman@gmail.com, or open up an issue on [our Github repo](https://github.com/dreisman/WebCensusNotebook).\n",
    "\n",
    "\n",
    "## Getting started with the Web Census data:\n",
    "\n",
    "Run the cell below to create a `Census` object, which encapsulates a connection to our PostgreSQL crawl database. This object provides the interface to interact with web census data — some examples of what you can do are located in the cells in the rest of this notebook\n",
    "\n",
    "(*Note*: Once a cell is selected, the keyboard shortcut to execute it is shift+enter!)\n",
    "\n",
    "Our  available census crawls are:\n",
    "* \"census_2016_08_25k_id_detection_1\" : A crawl of the top 25k sites, with browser state (cookies, localstorage, etc.) maintained between each site visit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.realpath('censuslib'))\n",
    "from censuslib import census\n",
    "from censuslib import utils\n",
    "from collections import defaultdict\n",
    "import psycopg2\n",
    "import re\n",
    "import dill\n",
    "\n",
    "# Note: If you'd like to access one of our other databases, replace census_name\n",
    "# with one of our other available crawls listed above\n",
    "census_name = 'census_2016_08_25k_id_detection_1'\n",
    "\n",
    "# the 'small_crawl' Census object provides the interface for interacting with\n",
    "# census data\n",
    "small_crawl = census.Census(census_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see that a given top_url is present in the dataset\n",
    "All data in the census is keyed by each 'top_url' visited. Each top_url follows the format:  \n",
    "\n",
    "`http://example.com`  \n",
    "\n",
    "There is never a leading '`www.`', nor is the scheme ever '`https://`'. If a site redirects to https, that will be reflected in the crawl's data.\n",
    "\n",
    "Before running one of the cells below, you may want to run this cell to see if a site you are interested in is present in our census crawl data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_url = 'http://netflix.com'\n",
    "print(small_crawl.check_top_url(top_url))\n",
    "\n",
    "top_url = 'http://notincensus.com'\n",
    "print(small_crawl.check_top_url(top_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all third party trackers on a site\n",
    "`census.get_all_third_party_trackers_by_site(top_url)` returns a list of third party resources loaded on the site's landing page (top_url) that were identified to be trackers.\n",
    "\n",
    "To determine if a URL is a tracker, we check the URL against two blocklists: the EasyList filter list, and the EasyPrivacy filter list, both provided by the [EasyList](https://easylist.to/) community. The EasyList filters identify resources that are used in advertising and is a popular list used by adblockers. The EasyPrivacy filters identify additional resource used in tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_url = 'http://espn.go.com'\n",
    "\n",
    "results = small_crawl.get_all_third_party_trackers_by_site('http://espn.go.com')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get all third party responses on a site\n",
    "For a more comprehensive view of the third party resources on a website, run this cell.\n",
    "\n",
    "`census.get_all_third_party_responses_by_site(top_url)` returns a two-level results dict containing third party urls loaded on the given site's landing page (top_url).\n",
    "\n",
    "The dict's structure is:\n",
    "\n",
    "`dict[third_party_url]['is_tracker']`, contains True if third_party_url is identified on a blocklist.  \n",
    "`dict[third_party_url]['is_js']`, contains True if third_party_url is a script.  \n",
    "`dict[third_party_url]['is_img']`, contains True if third_party_url is an image.  \n",
    "`dict[third_party_url]['url_domain']`, contains the domain of the third party.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = small_crawl.get_all_third_party_responses_by_site('http://espn.go.com')\n",
    "\n",
    "\n",
    "print('Example of one of the third parties:')\n",
    "third_party = results.popitem()\n",
    "\n",
    "print('Third party URI loaded on page: ' + third_party[0])\n",
    "print('Third party domain: ' + third_party[1]['url_domain'])\n",
    "print('Is it a tracker?: ' + str(third_party[1]['is_tracker']))\n",
    "print('Is it an image?: ' + str(third_party[1]['is_img']))\n",
    "print('Is it a script?: ' + str(third_party[1]['is_js']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all third party responses for a list of sites\n",
    "Run the cell below to fetch third party data for multiple sites. The results are written to CSV files in your home directory — visit the Jupyter Notebook file browser at https://webcensus.openwpm.com/results to view and download the files.\n",
    "\n",
    "The resulting CSVs are:\n",
    "\n",
    "* `tracker_js_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with tracking scripts on that site.\n",
    "\n",
    "* `non_tracker_js_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with non-tracking scripts on that site\n",
    "\n",
    "* `tracker_img_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with tracking images (pixels, beacons, ads, etc.) on that site.\n",
    "\n",
    "* `non_tracker_img_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with non-tracking images on that site\n",
    "\n",
    "* `tracker_other_by_site.csv` : A CSV of rows of [site, tp_domain] for domains of third-party resources that could not be identified as scripts or images, but were still identified as trackers\n",
    "\n",
    "* `non_tracker_other_by_site.csv` : A CSV of rows of [site, tp_domain] for domains of third-party resources that could not be identified as scripts, images, or trackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List sites to fetch data for :\n",
    "sites = ['http://cnn.com', 'http://wsj.com']\n",
    "\n",
    "# This function will write the results to multiple CSVs\n",
    "small_crawl.get_third_party_resources_for_multiple_sites(sites, filepath='results/')\n",
    "\n",
    "print(\"Check file browser at https://webcensus.openwpm.com/results to see output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sites that load a resource from a given third party domain\n",
    "\n",
    "`census.get_top_urls_with_third_party_domain(tp_domain)` returns a dictionary mapping sites in the census to a list of resources loaded on that site belonging to that tp_domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp_domain = 'addthis.com'\n",
    "\n",
    "tps_by_top = small_crawl.get_sites_with_third_party_domain(tp_domain)\n",
    "\n",
    "print(\"Number of sites with given third party : \" + str(len(tps_by_top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"cookie sync\" events on a given site\n",
    "\"Cookie syncing\" is a practice that allows one third party to share the value of a user's cookie with a different third party. The practice is described in [this Freedom-to-Tinker blogpost](https://freedom-to-tinker.com/2014/08/07/the-hidden-perils-of-cookie-syncing/). Sharing identifiers with other third parties allows third parties to expand the tracking services they can provide to customers.\n",
    "\n",
    "\n",
    "Use `get_cookie_syncs_by_site(site, cookie_length)` to get a dictionary mapping third-party domains to tuples of (domain, cookie_value) that share a cookie with that third-party.\n",
    "\n",
    "Note: This does not include logic for isolating \"identifying cookies.\" Any cookies of a sufficient cookie length that are shared with other domains will be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The in order for a cookie to be considered cookie syncing,\n",
    "# it must be at least cookie_length characters long\n",
    "results = small_crawl.get_cookie_syncs_by_site('http://microsoft.com', cookie_length=8)\n",
    "\n",
    "for receiving_url in results:\n",
    "    print(\"Domain receiving cookie: \" + receiving_url)\n",
    "    for sending_url, val in results[receiving_url]:\n",
    "        print(\"\\tDomain sharing cookie: \" + sending_url)\n",
    "        print(\"\\tCookie value: \" + val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"cookie sync\" events on multiple sites\n",
    "Run this cell to get data on cookie syncing from multiple sites.  The results are written to CSV files in your home directory — visit the Jupyter Notebook file browser at https://webcensus.openwpm.com to view and download the files.\n",
    "\n",
    "The resulting CSV files are:\n",
    "\n",
    "* `condensed_cookie_syncs.csv` : A CSV containing domains of third-parties receiving cookies and the domains sharing cookies, by site.\n",
    "* `full_cookie_syncs.csv` : A CSV with rows containing the URL used to sync a cookie, the domain sharing the cookie, the cookie value, and the site the cookie sync took place on.\n",
    "\n",
    "\n",
    "*Note*: This function can be a bit slow for a longer list of sites, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites = ['http://microsoft.com', 'http://cnn.com']\n",
    "\n",
    "small_crawl.get_cookie_syncs_for_multiple_sites(sites, filepath='results/')\n",
    "\n",
    "print(\"Check file browser at https://webcensus.openwpm.com/results to see output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a given url against a blocklist\n",
    "Run the cell below to check a url against one of two blocklists.\n",
    "\n",
    "Available blocklists (described above):\n",
    "* `easylist.txt`\n",
    "* `easyprivacy.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_to_check = 'http://tags.bkrtx.com/js/bk-coretag.js'\n",
    "\n",
    "# Set is_js if url is known to be a script resource (or unset if unknown).\n",
    "# Set is_img if url is known to be an image (or unset if unknown).\n",
    "# If first_party site on which URL appeared is known, provide it as first_party.\n",
    "# Else, leave unset.\n",
    "utils.is_tracker(url_to_check, is_js=True, is_img=False, \n",
    "                 first_party='http://verizonwireless.com', blocklist='easylist.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the domain of a given url\n",
    "The cell below strips a URL down to just it's domain ([public suffix](https://publicsuffix.org/) + hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.get_domain('http://subdomain.example.com/this/will/be/deleted.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
