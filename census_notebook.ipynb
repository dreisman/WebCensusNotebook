{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import utils\n",
    "import re\n",
    "import census\n",
    "from collections import defaultdict\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up: Connect to the database\n",
    "Two databases available:\n",
    "* census_2016_04_spider_4\n",
    "* census_2016_08_25k_id_detection_1\n",
    "\n",
    "This cell will set the 'con' object â€” the database connection. It gets passed to all census functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "census_name = 'census_2016_08_25k_id_detection_1' # Change me!\n",
    "\n",
    "small_crawl = census.Census(census_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see that a given top_url is present in the dataset\n",
    "All data in the census is keyed by each 'top_url' visited. Each top_url follows the format:  \n",
    "\n",
    "`http://example.com`  \n",
    "\n",
    "There is never a leading '`www.`', nor is the scheme ever '`https://`'. If a site redirects to https, that will be reflected in the crawl's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_url = 'http://netflix.com'\n",
    "print(small_crawl.check_top_url(top_url))\n",
    "\n",
    "top_url = 'http://notincensus.com'\n",
    "print(small_crawl.check_top_url(top_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get all third party responses by top_url\n",
    "`census.get_third_party_responses_by_domain(con, top_url)` returns a two-level results dict containing third party urls loaded on the given top_url.\n",
    "\n",
    "The dict's structure is:\n",
    "\n",
    "`dict[third_party_url]['is_tracker']`, contains True if third_party_url is identified on a blocklist.  \n",
    "`dict[third_party_url]['is_js']`, contains True if third_party_url is a script.  \n",
    "`dict[third_party_url]['is_img']`, contains True if third_party_url is an image.  \n",
    "`dict[third_party_url]['url_ps']`, contains the string for the public-suffix+1 of the url.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_url = 'http://espn.go.com'\n",
    "\n",
    "results = small_crawl.get_third_party_responses_by_domain('http://espn.go.com')\n",
    "\n",
    "third_party_trackers = {results[x]['url_ps'] for x in results if results[x]['is_tracker']}\n",
    "\n",
    "print(\"Number of third_party trackers on domain: \" + str(len(third_party_trackers)))\n",
    "\n",
    "for url in results:\n",
    "    print(url)\n",
    "    print('\\tIs a script? ' + str(results[url]['is_js']))\n",
    "    print('\\tIs a tracker? ' + str(results[url]['is_tracker']))\n",
    "    print('\\tPS+1: ' + results[url]['url_ps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all third party responses for a list of sites\n",
    "A cell to simplify getting all third party responses for a given list of sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sites = ['http://cnn.com', 'http://wsj.com'] # Change me!\n",
    "\n",
    "# Resulting dictionaries\n",
    "tracker_js_by_top = defaultdict(set)\n",
    "tracker_img_by_top = defaultdict(set)\n",
    "non_tracker_js_by_top = defaultdict(set)\n",
    "non_tracker_img_by_top = defaultdict(set)\n",
    "\n",
    "tracker_other_by_top = defaultdict(set)\n",
    "non_tracker_other_by_top = defaultdict(set)\n",
    "for site in sites:\n",
    "    tp_data = small_crawl.get_third_party_responses_by_domain(site)\n",
    "    for url in tp_data:\n",
    "        url_ps = tp_data[url]['url_ps']\n",
    "        is_tracker = tp_data[url]['is_tracker']\n",
    "        if is_tracker:\n",
    "            if tp_data[url]['is_js']:\n",
    "                tracker_js_by_top[site].add(url_ps)\n",
    "            elif tp_data[url]['is_img']:\n",
    "                tracker_img_by_top[site].add(url_ps)\n",
    "            else:\n",
    "                tracker_other_by_top[site].add(url_ps)\n",
    "        else:\n",
    "            if tp_data[url]['is_js']:\n",
    "                non_tracker_js_by_top[site].add(url_ps)\n",
    "            elif tp_data[url]['is_img']:\n",
    "                non_tracker_img_by_top[site].add(url_ps)           \n",
    "            else:\n",
    "                non_tracker_other_by_top[site].add(url_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save output as .dill\n",
    "\n",
    "with open('tracker_js_by_top.dill', 'wb') as f:\n",
    "    dill.dump(tracker_js_by_top, f)\n",
    "with open('tracker_js_by_top.dill', 'wb') as f:\n",
    "    dill.dump(non_tracker_js_by_top, f)\n",
    "with open('tracker_img_by_top.dill', 'wb') as f:\n",
    "    dill.dump(tracker_img_by_top, f)\n",
    "with open('non_tracker_img_by_top.dill', 'wb') as f:\n",
    "    dill.dump(non_tracker_img_by_top, f)\n",
    "with open('tracker_other_by_top.dill', 'wb') as f:\n",
    "    dill.dump(tracker_other_by_top, f)\n",
    "with open('non_tracker_other_by_top.dill', 'wb') as f:\n",
    "    dill.dump(non_tracker_other_by_top, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save output in CSVs\n",
    "with open('tracker_js_by_top.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'tp_domain'])\n",
    "    for top in tracker_js_by_top:\n",
    "        for tp in tracker_js_by_top[top]:\n",
    "            writer.writerow([top, tp])\n",
    "with open('non_tracker_js_by_top.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'tp_domain'])\n",
    "    for top in non_tracker_js_by_top:\n",
    "        for tp in non_tracker_js_by_top[top]:\n",
    "            writer.writerow([top, tp])\n",
    "with open('tracker_img_by_top.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'tp_domain'])\n",
    "    for top in tracker_img_by_top:\n",
    "        for tp in tracker_img_by_top[top]:\n",
    "            writer.writerow([top, tp])\n",
    "with open('non_tracker_img_by_top.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'tp_domain'])\n",
    "    for top in non_tracker_img_by_top:\n",
    "        for tp in non_tracker_img_by_top[top]:\n",
    "            writer.writerow([top, tp])\n",
    "with open('tracker_other_by_top.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'tp_domain'])\n",
    "    for top in tracker_other_by_top:\n",
    "        for tp in tracker_other_by_top[top]:\n",
    "            writer.writerow([top, tp])\n",
    "with open('non_tracker_other_by_top.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'tp_domain'])\n",
    "    for top in non_tracker_other_by_top:\n",
    "        for tp in non_tracker_other_by_top[top]:\n",
    "            writer.writerow([top, tp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top_urls that load a resource from a given third party domain\n",
    "\n",
    "`census.get_top_urls_with_third_party_domain(con, tp_domain)` returns a dictionary mapping top_urls in the census to a list of urls that were loaded on that top_url belonging to a certain tp_domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp_domain = 'addthis.com'\n",
    "\n",
    "tps_by_top = small_crawl.get_top_urls_with_third_party_domain(tp_domain)\n",
    "\n",
    "print(\"Number of top_urls with given third party : \" + str(len(tps_by_top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"cookie sync\" events on a given top_url\n",
    "Note: This does not include logic for isolating \"identifying cookies.\" Any cookies of a sufficient cookie length that are shared with other domains will be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a single top_url...\n",
    "\n",
    "results = small_crawl.get_cookie_syncs_v2('http://microsoft.com', cookie_length=8)\n",
    "\n",
    "for receiving_url in results:\n",
    "    print(\"R: \" + receiving_url)\n",
    "    for sending_url, val in results[receiving_url]:\n",
    "        print(\"\\tS: \" + sending_url)\n",
    "        print(\"\\tV: \" + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a list of top_urls...\n",
    "# Warning: this method is slow.\n",
    "\n",
    "sites = ['http://microsoft.com', 'http://cnn.com']  # Change me!\n",
    "\n",
    "cookie_sync_data = defaultdict(defaultdict)\n",
    "for i, site in sites:\n",
    "    print(site)\n",
    "    cookie_sync_data[site] = small_crawl.get_cookie_syncs_v2(site, cookie_length=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write output as .dill\n",
    "with open('cookie_syncs.dill', 'w') as f:\n",
    "    dill.dump(cookie_sync_data, f)\n",
    "\n",
    "# Write complete output as csv\n",
    "with open('cookie_syncs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'sending_domain', 'receiving_url', 'cookie_value'])\n",
    "    for site in cookie_sync_data:\n",
    "        for receiving_url in cookie_sync_data[site]:\n",
    "            for sending_url, cookie_value in cookie_sync_data[site][receiving_url]:\n",
    "                writer.writerow([site, sending_url, receiving_url, cookie_value])\n",
    "\n",
    "# Write partial output as CSV, only identifying sending domain and receiving domain\n",
    "# (rather than the full receiving URL)\n",
    "\n",
    "cooks_just_domains = defaultdict(defaultdict)\n",
    "for site in cookie_sync_data:\n",
    "    cooks_just_domains[site] = defaultdict(set)\n",
    "    for receiving_url in cookie_sync_data[site]:\n",
    "        for sending_domain, value in cookie_sync_data[site][receiving_url]:\n",
    "            cooks_just_domains[site][utils.get_host_plus_ps(receiving_url)].add(sending_domain)\n",
    "with open('../wsj/cookie_syncs_v2_just_domains.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'sending_domain', 'receiving_domain'])\n",
    "    for site in cooks_just_domains:\n",
    "        for receiving_domain in cooks_just_domains[site]:\n",
    "            if len(cooks_just_domains[site][receiving_domain]) > 1 and 'NOT_FOUND' in cooks_just_domains[site][receiving_domain]:\n",
    "                cooks_just_domains[site][receiving_domain].discard('NOT_FOUND')\n",
    "            for sending_domain in cooks_just_domains[site][receiving_domain]:\n",
    "                writer.writerow([site, sending_domain, receiving_domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a given url against a blocklist\n",
    "Available blocklists:\n",
    "* easylist.txt\n",
    "* easyprivacy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(utils.is_tracker('http://tags.bkrtx.com/js/bk-coretag.js', is_js=True, is_img=False, \n",
    "                       first_party='http://verizonwireless.com', blocklist='easylist.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get third party scripts on given top_url that call particular javascript symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(small_crawl.get_urls_with('http://cnn.com', 'CanvasRenderingContext2D.fillText'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the PS+1 of a given top_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(utils.get_host_plus_ps('http://subdomain.example.com/this/will/be/deleted.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
