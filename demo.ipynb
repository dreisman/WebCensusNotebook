{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Princeton Web Census Interface\n",
    "## About\n",
    "This jupyter notebook provides a demo for interacting with data from the Princeton Web Census.\n",
    "\n",
    "The code relies on our utilities located in the censuslib/ directory, which you can use for your own experiments.\n",
    "\n",
    "Each 'cell' in this notebook represents a different capability of our census interface. Each can be executed separately from the others, but first you must start with the following instructions...\n",
    "\n",
    "\n",
    "## Getting started with the Web Census data:\n",
    "\n",
    "Run the cell below to create a `Census` object, which encapsulates a connection to our PostgreSQL crawl database. This object provides the interface to interact with web census data — some examples of what you can do are located in the cells in the rest of this notebook\n",
    "\n",
    "(*Note*: the keyboard shortcut to run a cell is shift+enter!)\n",
    "\n",
    "Our  available census crawls are:\n",
    "* \"census_2016_08_25k_id_detection_1\" : A crawl of the top 25k sites, with browser state (cookies, localstorage, etc.) maintained between each site visit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.realpath('censuslib'))\n",
    "from censuslib import census\n",
    "from censuslib import utils\n",
    "from collections import defaultdict\n",
    "import psycopg2\n",
    "import re\n",
    "import dill\n",
    "\n",
    "# Note: If you'd like to access one of our other databases, replace census_name\n",
    "# with one of our other available crawls listed above\n",
    "census_name = 'census_2016_08_25k_id_detection_1'\n",
    "\n",
    "# the 'small_crawl' Census object provides the interface for interacting with\n",
    "# census data\n",
    "small_crawl = census.Census(census_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see that a given top_url is present in the dataset\n",
    "All data in the census is keyed by each 'top_url' visited. Each top_url follows the format:  \n",
    "\n",
    "`http://example.com`  \n",
    "\n",
    "There is never a leading '`www.`', nor is the scheme ever '`https://`'. If a site redirects to https, that will be reflected in the crawl's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_url = 'http://netflix.com'\n",
    "print(small_crawl.check_top_url(top_url))\n",
    "\n",
    "top_url = 'http://notincensus.com'\n",
    "print(small_crawl.check_top_url(top_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all third party trackers on a site\n",
    "`census.get_all_third_party_trackers_by_site(top_url)` returns a list of third party resources loaded on the site's landing page (top_url) that were identified to be trackers.\n",
    "\n",
    "To determine if a URL is a tracker, we check the URL against two blocklists: the EasyList filter list, and the EasyPrivacy filter list, both provided by the [EasyList](https://easylist.to/) community. The EasyList filters identify resources that are used in advertising and is a popular list used by adblockers. The EasyPrivacy filters identify additional resource used in tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_url = 'http://espn.go.com'\n",
    "\n",
    "results = small_crawl.get_all_third_party_trackers_by_site('http://espn.go.com')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get all third party responses on a site\n",
    "For a more comprehensive view of the third party resources on a website, run this cell.\n",
    "\n",
    "`census.get_all_third_party_responses_by_site(top_url)` returns a two-level results dict containing third party urls loaded on the given site's landing page (top_url).\n",
    "\n",
    "The dict's structure is:\n",
    "\n",
    "`dict[third_party_url]['is_tracker']`, contains True if third_party_url is identified on a blocklist.  \n",
    "`dict[third_party_url]['is_js']`, contains True if third_party_url is a script.  \n",
    "`dict[third_party_url]['is_img']`, contains True if third_party_url is an image.  \n",
    "`dict[third_party_url]['url_domain']`, contains the domain of the third party.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = small_crawl.get_all_third_party_responses_by_site('http://espn.go.com')\n",
    "\n",
    "\n",
    "print('Example of one of the third parties:')\n",
    "third_party = results.popitem()\n",
    "\n",
    "print('Third party URI loaded on page: ' + third_party[0])\n",
    "print('Third party domain: ' + third_party[1]['url_domain'])\n",
    "print('Is it a tracker?: ' + str(third_party[1]['is_tracker']))\n",
    "print('Is it an image?: ' + str(third_party[1]['is_img']))\n",
    "print('Is it a script?: ' + str(third_party[1]['is_js']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all third party responses for a list of sites\n",
    "Run the cell below to fetch third party data for multiple sites. The results are printed to CSV files in your home directory — visit the Jupyter Notebook file browser at https://webcensus.openwpm.com to view and download the files.\n",
    "\n",
    "The resulting CSVs are:\n",
    "\n",
    "* `tracker_js_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with tracking scripts on that site.\n",
    "\n",
    "* `non_tracker_js_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with non-tracking scripts on that site\n",
    "\n",
    "* `tracker_img_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with tracking images (pixels, beacons, ads, etc.) on that site.\n",
    "\n",
    "* `non_tracker_img_by_site.csv` : A CSV of rows of [site, tp_domain] for third-party domains with non-tracking images on that site\n",
    "\n",
    "* `tracker_other_by_site.csv` : A CSV of rows of [site, tp_domain] for domains of third-party resources that could not be identified as scripts or images, but were still identified as trackers\n",
    "\n",
    "* `non_tracker_other_by_site.csv` : A CSV of rows of [site, tp_domain] for domains of third-party resources that could not be identified as scripts, images, or trackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results written to filesystem. Check file browser at https://webcensus.openwpm.com to see results.\n"
     ]
    }
   ],
   "source": [
    "# List sites to fetch data for :\n",
    "sites = ['http://cnn.com', 'http://wsj.com']\n",
    "\n",
    "# This function will write the results to multiple CSVs\n",
    "small_crawl.get_third_party_resources_for_multiple_sites(sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top_urls that load a resource from a given third party domain\n",
    "\n",
    "`census.get_top_urls_with_third_party_domain(con, tp_domain)` returns a dictionary mapping top_urls in the census to a list of urls that were loaded on that top_url belonging to a certain tp_domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp_domain = 'addthis.com'\n",
    "\n",
    "tps_by_top = small_crawl.get_sites_with_third_party_domain(tp_domain)\n",
    "\n",
    "print(\"Number of top_urls with given third party : \" + str(len(tps_by_top)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = small_crawl.connection.cursor(\"urls-rewrite\")\n",
    "c.itersize = 100000\n",
    "\n",
    "c.execute('SELECT * FROM urls')\n",
    "print('executed!')\n",
    "counter = 0\n",
    "for row in c:\n",
    "    counter += 1\n",
    "    #print(row)\n",
    "    if counter % 100 == 0:\n",
    "        print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"cookie sync\" events on a given top_url\n",
    "Note: This does not include logic for isolating \"identifying cookies.\" Any cookies of a sufficient cookie length that are shared with other domains will be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a single top_url...\n",
    "\n",
    "results = small_crawl.get_cookie_syncs_v2('http://microsoft.com', cookie_length=8)\n",
    "\n",
    "for receiving_url in results:\n",
    "    print(\"R: \" + receiving_url)\n",
    "    for sending_url, val in results[receiving_url]:\n",
    "        print(\"\\tS: \" + sending_url)\n",
    "        print(\"\\tV: \" + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a list of top_urls...\n",
    "# Warning: this method is slow.\n",
    "\n",
    "sites = ['http://microsoft.com', 'http://cnn.com']  # Change me!\n",
    "\n",
    "cookie_sync_data = defaultdict(defaultdict)\n",
    "for i, site in sites:\n",
    "    print(site)\n",
    "    cookie_sync_data[site] = small_crawl.get_cookie_syncs_v2(site, cookie_length=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write output as .dill\n",
    "with open('cookie_syncs.dill', 'w') as f:\n",
    "    dill.dump(cookie_sync_data, f)\n",
    "\n",
    "# Write complete output as csv\n",
    "with open('cookie_syncs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'sending_domain', 'receiving_url', 'cookie_value'])\n",
    "    for site in cookie_sync_data:\n",
    "        for receiving_url in cookie_sync_data[site]:\n",
    "            for sending_url, cookie_value in cookie_sync_data[site][receiving_url]:\n",
    "                writer.writerow([site, sending_url, receiving_url, cookie_value])\n",
    "\n",
    "# Write partial output as CSV, only identifying sending domain and receiving domain\n",
    "# (rather than the full receiving URL)\n",
    "\n",
    "cooks_just_domains = defaultdict(defaultdict)\n",
    "for site in cookie_sync_data:\n",
    "    cooks_just_domains[site] = defaultdict(set)\n",
    "    for receiving_url in cookie_sync_data[site]:\n",
    "        for sending_domain, value in cookie_sync_data[site][receiving_url]:\n",
    "            cooks_just_domains[site][utils.get_host_plus_ps(receiving_url)].add(sending_domain)\n",
    "with open('../wsj/cookie_syncs_v2_just_domains.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['top_url', 'sending_domain', 'receiving_domain'])\n",
    "    for site in cooks_just_domains:\n",
    "        for receiving_domain in cooks_just_domains[site]:\n",
    "            if len(cooks_just_domains[site][receiving_domain]) > 1 and 'NOT_FOUND' in cooks_just_domains[site][receiving_domain]:\n",
    "                cooks_just_domains[site][receiving_domain].discard('NOT_FOUND')\n",
    "            for sending_domain in cooks_just_domains[site][receiving_domain]:\n",
    "                writer.writerow([site, sending_domain, receiving_domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a given url against a blocklist\n",
    "Available blocklists:\n",
    "* easylist.txt\n",
    "* easyprivacy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(utils.is_tracker('http://tags.bkrtx.com/js/bk-coretag.js', is_js=True, is_img=False, \n",
    "                       first_party='http://verizonwireless.com', blocklist='easylist.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get third party scripts on given top_url that call particular javascript symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(small_crawl.get_urls_with('http://cnn.com', 'CanvasRenderingContext2D.fillText'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the domain of a given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(utils.get_domain('http://subdomain.example.com/this/will/be/deleted.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.close()\n",
    "small_crawl.connection.rollback()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
